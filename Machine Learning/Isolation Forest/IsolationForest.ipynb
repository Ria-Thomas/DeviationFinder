{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------+-------------------+\n",
      "|           timestamp|       Z|   Zscale|         timestamp1|\n",
      "+--------------------+--------+---------+-------------------+\n",
      "|2018-07-11 09:04:...|0.785156| 0.039606|2018-07-11 09:04:00|\n",
      "|2018-07-11 09:04:...| 0.78125| -0.07135|2018-07-11 09:04:00|\n",
      "|2018-07-11 09:04:...|0.785156| 0.039606|2018-07-11 09:04:00|\n",
      "|2018-07-11 09:04:...|0.792969| 0.261546|2018-07-11 09:04:00|\n",
      "|2018-07-11 09:04:...|0.785156| 0.039606|2018-07-11 09:04:00|\n",
      "|2018-07-11 09:04:...|0.773438|-0.293261|2018-07-11 09:04:00|\n",
      "|2018-07-11 09:04:...|0.785156| 0.039606|2018-07-11 09:04:00|\n",
      "|2018-07-11 09:04:...|0.792969| 0.261546|2018-07-11 09:04:00|\n",
      "|2018-07-11 09:04:...|0.789063|  0.15059|2018-07-11 09:04:00|\n",
      "|2018-07-11 09:04:...|0.777344|-0.182306|2018-07-11 09:04:00|\n",
      "|2018-07-11 09:04:...|0.761719|-0.626157|2018-07-11 09:04:00|\n",
      "|2018-07-11 09:04:...|    0.75|-0.959053|2018-07-11 09:04:00|\n",
      "|2018-07-11 09:04:...|0.710938|-2.068667|2018-07-11 09:04:00|\n",
      "|2018-07-11 09:04:...|0.675781|-3.067354|2018-07-11 09:04:00|\n",
      "|2018-07-11 09:04:...|0.667969|-3.289266|2018-07-11 09:04:00|\n",
      "|2018-07-11 09:04:...|0.683594|-2.845414|2018-07-11 09:04:00|\n",
      "|2018-07-11 09:04:...|0.695313|-2.512519|2018-07-11 09:04:00|\n",
      "|2018-07-11 09:04:...|0.707031|-2.179651|2018-07-11 09:04:00|\n",
      "|2018-07-11 09:04:...|0.726563|-1.624816|2018-07-11 09:04:00|\n",
      "|2018-07-11 09:04:...|0.714844|-1.957711|2018-07-11 09:04:00|\n",
      "+--------------------+--------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Scaled Data\n",
      "\n",
      "+-------------------+--------+---------+-------------------+\n",
      "|         timestamp1|       Z|   Zscale|movingAverage_round|\n",
      "+-------------------+--------+---------+-------------------+\n",
      "|2018-07-11 09:04:15|0.796875| 0.372502|                0.0|\n",
      "|2018-07-11 09:04:15|0.785156| 0.039606|                0.0|\n",
      "|2018-07-11 09:04:15|0.789063|  0.15059|                0.0|\n",
      "|2018-07-11 09:04:15|0.785156| 0.039606|                0.0|\n",
      "|2018-07-11 09:04:15|0.777344|-0.182306|                0.0|\n",
      "|2018-07-11 09:04:15|0.789063|  0.15059|                0.0|\n",
      "|2018-07-11 09:04:15|0.789063|  0.15059|                0.0|\n",
      "|2018-07-11 09:04:15|0.789063|  0.15059|                0.0|\n",
      "|2018-07-11 09:04:15| 0.78125| -0.07135|                0.0|\n",
      "|2018-07-11 09:04:15| 0.78125| -0.07135|                0.0|\n",
      "|2018-07-11 09:04:15|0.777344|-0.182306|                0.0|\n",
      "|2018-07-11 09:04:15|0.785156| 0.039606|                0.0|\n",
      "|2018-07-11 09:04:15|0.785156| 0.039606|                0.0|\n",
      "|2018-07-11 09:04:15|0.785156| 0.039606|                0.0|\n",
      "|2018-07-11 09:04:15|0.789063|  0.15059|                0.0|\n",
      "|2018-07-11 09:04:15|0.777344|-0.182306|                0.0|\n",
      "|2018-07-11 09:04:15|0.789063|  0.15059|                0.0|\n",
      "|2018-07-11 09:04:15|0.785156| 0.039606|                0.0|\n",
      "|2018-07-11 09:04:15|0.785156| 0.039606|                0.0|\n",
      "|2018-07-11 09:04:15| 0.78125| -0.07135|                0.0|\n",
      "+-------------------+--------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+\n",
      "|features|\n",
      "+--------+\n",
      "|   [0.0]|\n",
      "|   [0.0]|\n",
      "|   [0.0]|\n",
      "|   [0.0]|\n",
      "|   [0.0]|\n",
      "|   [0.0]|\n",
      "|   [0.0]|\n",
      "|   [0.0]|\n",
      "|   [0.0]|\n",
      "|   [0.0]|\n",
      "|   [0.0]|\n",
      "|   [0.0]|\n",
      "|   [0.0]|\n",
      "|   [0.0]|\n",
      "|   [0.0]|\n",
      "|   [0.0]|\n",
      "|   [0.0]|\n",
      "|   [0.0]|\n",
      "|   [0.0]|\n",
      "|   [0.0]|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "1521\n",
      "True\n",
      "+--------+-------------------+----------+\n",
      "|features|       anomalyScore|prediction|\n",
      "+--------+-------------------+----------+\n",
      "|   [0.0]|0.45465342295099287|       0.0|\n",
      "|   [0.0]|0.45465342295099287|       0.0|\n",
      "|   [0.0]|0.45465342295099287|       0.0|\n",
      "|   [0.0]|0.45465342295099287|       0.0|\n",
      "|   [0.0]|0.45465342295099287|       0.0|\n",
      "|   [0.0]|0.45465342295099287|       0.0|\n",
      "|   [0.0]|0.45465342295099287|       0.0|\n",
      "|   [0.0]|0.45465342295099287|       0.0|\n",
      "|   [0.0]|0.45465342295099287|       0.0|\n",
      "|   [0.0]|0.45465342295099287|       0.0|\n",
      "|   [0.0]|0.45465342295099287|       0.0|\n",
      "|   [0.0]|0.45465342295099287|       0.0|\n",
      "|   [0.0]|0.45465342295099287|       0.0|\n",
      "|   [0.0]|0.45465342295099287|       0.0|\n",
      "|   [0.0]|0.45465342295099287|       0.0|\n",
      "|   [0.0]|0.45465342295099287|       0.0|\n",
      "|   [0.0]|0.45465342295099287|       0.0|\n",
      "|   [0.0]|0.45465342295099287|       0.0|\n",
      "|   [0.0]|0.45465342295099287|       0.0|\n",
      "|   [0.0]|0.45465342295099287|       0.0|\n",
      "+--------+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "#(1.0 means anomalous/ outlier, 0.0 normal/ inlier)\n",
      "Now in main\n",
      "\n",
      "+----------+-----------------+\n",
      "|prediction|count(prediction)|\n",
      "+----------+-----------------+\n",
      "|       1.0|               16|\n",
      "|       0.0|             1505|\n",
      "+----------+-----------------+\n",
      "\n",
      "+-------------------+----------+-------------------+\n",
      "|movingAverage_round|prediction|       anomalyScore|\n",
      "+-------------------+----------+-------------------+\n",
      "|                0.0|       0.0|0.45465342295099287|\n",
      "|                0.0|       0.0|0.45465342295099287|\n",
      "|                0.0|       0.0|0.45465342295099287|\n",
      "|                0.0|       0.0|0.45465342295099287|\n",
      "|                0.0|       0.0|0.45465342295099287|\n",
      "|                0.0|       0.0|0.45465342295099287|\n",
      "|                0.0|       0.0|0.45465342295099287|\n",
      "|                0.0|       0.0|0.45465342295099287|\n",
      "|                0.0|       0.0|0.45465342295099287|\n",
      "|                0.0|       0.0|0.45465342295099287|\n",
      "|                0.0|       0.0|0.45465342295099287|\n",
      "|                0.0|       0.0|0.45465342295099287|\n",
      "|                0.0|       0.0|0.45465342295099287|\n",
      "|                0.0|       0.0|0.45465342295099287|\n",
      "|                0.0|       0.0|0.45465342295099287|\n",
      "|                0.0|       0.0|0.45465342295099287|\n",
      "|                0.0|       0.0|0.45465342295099287|\n",
      "|                0.0|       0.0|0.45465342295099287|\n",
      "|                0.0|       0.0|0.45465342295099287|\n",
      "|                0.0|       0.0|0.45465342295099287|\n",
      "+-------------------+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Isolation Forest\n",
    "\n",
    "#Before Training Install the Spark -Iforest pacakage as mentioned in the link: https://github.com/titicaca/spark-iforest\n",
    "#installation reference 2: https://towardsdatascience.com/isolation-forest-and-spark-b88ade6c63ff\n",
    "\n",
    "#packages\n",
    "import sys\n",
    "from pyspark.sql import SparkSession, functions, types\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col, mean, stddev,stddev_pop,avg,max,to_timestamp,udf,desc\n",
    "from pyspark.sql import Window\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark_iforest.ml.iforest import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "\n",
    "\n",
    "# Configure Spark\n",
    "conf = SparkConf()\n",
    "conf.set('spark.jars', '/Users/varnnithavinay/spark-iforest/target/spark-iforest-2.4.0.jar')\n",
    "spark = SparkSession \\\n",
    "  .builder \\\n",
    "  .appName(\"Python Spark SQL basic example\") \\\n",
    "  .config(\"spark.memory.fraction\", 0.8) \\\n",
    "  .config(\"spark.executor.memory\", \"25G\") \\\n",
    "  .config(\"spark.driver.memory\", \"25G\")\\\n",
    "  .config(\"spark.sql.shuffle.partitions\" , \"80\") \\\n",
    "  .config(\"spark.memory.offHeap.enabled\",'true')\\\n",
    "  .config(\"spark.memory.offHeap.size\",\"25G\")\\\n",
    "  .getOrCreate()\n",
    "# UDF for converting column type from vector to double type\n",
    "unlist = udf(lambda x: round(float(list(x)[0]),6), types.DoubleType())\n",
    "\n",
    "##OTHER FUNCTIONS\n",
    "\n",
    "# Schema Set\n",
    "# Defining the schema for tmax datasets\n",
    "def sensor_schema():\n",
    "    sen_schema = types.StructType([\n",
    "        types.StructField('timestamp', types.StringType()),\n",
    "        types.StructField('X', types.DoubleType()),\n",
    "        types.StructField('Y', types.DoubleType()),\n",
    "        types.StructField('Z', types.DoubleType()),\n",
    "    ])\n",
    "    return sen_schema\n",
    "     \n",
    "## Standard Scaler - Z normalization\n",
    "def z_norm(temp):\n",
    "    assembler = VectorAssembler(\n",
    "    inputCols=[\"Z\"],\n",
    "    outputCol=\"Zvector\")\n",
    "    tempdata = assembler.transform(temp)\n",
    "    scaler = StandardScaler(inputCol=\"Zvector\", outputCol=\"Zscale\",withMean=True, withStd=True)\n",
    "    scalerModel = scaler.fit(tempdata)\n",
    "    scaledData = scalerModel.transform(tempdata).withColumn(\"Zscale\", unlist(\"Zscale\")).drop(\"Zvector\").cache()\n",
    "    scaledData.createOrReplaceTempView(\"scaledData\")\n",
    "    return scaledData\n",
    "\n",
    "def movingAverage(scaledData):\n",
    "    movAvg = scaledData.withColumn(\"movingAverage\", avg(scaledData[\"Zscale\"])\n",
    "             .over( Window.partitionBy(scaledData[\"timestamp1\"]).rowsBetween(-3,3))).cache()\n",
    "    return movAvg\n",
    "\n",
    "  \n",
    "#to save the model\n",
    "  \n",
    "temp_path = '/Users/varnnithavinay/Desktop/'\n",
    "iforest_path = temp_path + \"/iforest\"\n",
    "model_path = temp_path + \"/iforest_model\"\n",
    "\n",
    "########################################################\n",
    "\n",
    "def iforest_model(features):\n",
    "    \n",
    "    iforest = IForest(contamination=0.015, maxDepth=2)\n",
    "    iforest.setSeed(43)  # for reproducibility\n",
    "\n",
    "    model = iforest.fit(features)\n",
    "    # Check if the model has summary or not, the newly trained model has the summary info\n",
    "    print(model.hasSummary)\n",
    "\n",
    "    #Show the number of anomalies\n",
    "    summary = model.summary\n",
    "    #print(summary.numAnomalies)\n",
    "    \n",
    "    # Predict for a new data frame based on the fitted model\n",
    "    transformed = model.transform(features)\n",
    "\n",
    "    # Save the iforest estimator into the path\n",
    "    iforest.save(iforest_path)\n",
    "\n",
    "    # Load iforest estimator from a path\n",
    "    loaded_iforest = IForest.load(iforest_path)\n",
    "\n",
    "    # Save the fitted model into the model path\n",
    "    model.save(model_path)\n",
    "\n",
    "    # Load a fitted model from a model path\n",
    "    loaded_model = IForestModel.load(model_path)\n",
    "\n",
    "    # The loaded model has no summary info\n",
    "    #print(loaded_model.hasSummary)\n",
    "\n",
    "    # Use the loaded model to predict a new data frame\n",
    "    \n",
    "    return loaded_model.transform(features)\n",
    "\n",
    "    \n",
    "\n",
    "########################################################\n",
    "\n",
    "\n",
    "def main(inputs):\n",
    "    #Spark read of data\n",
    "    sens_schema = sensor_schema()\n",
    "    #for training in the original data set use lines below:\n",
    "    #temp1 = spark.read.csv(inputs, schema =sens_schema ).repartition(100)\n",
    "        # Initial Filter to obtain useful Data. Other ranges are useless\n",
    "    #temp.createOrReplaceTempView(\"temp\")\n",
    "    #temp = spark.sql(\"select timestamp,(Y*-1) as Y from temp where timestamp between '2018-07-09 12:00:00' and '2018-07-09 14:00:00'\")\n",
    "    #temp.show()\n",
    "    #scaledData  = z_norm(temp)\n",
    "    #scaledData = scaledData.withColumn(\"timestamp1\", to_timestamp(\"timestamp\", 'yyyy-MM-dd HH:mm:ss')).cache()\n",
    "    #movAvg = movingAverage(scaledData)\n",
    "    #movAvg.createOrReplaceTempView(\"movAvg\")\n",
    "        #Select only rounded values\n",
    "    #scaledNorm =  spark.sql('SELECT timestamp1,Z,Zscale,round(movingAverage,0) as movingAverage_round from movAvg').cache()\n",
    "    \n",
    "    #training sample\n",
    "    temp1 = spark.read.csv(inputs,header = True)\n",
    "    temp1.createOrReplaceTempView(\"temp1\")\n",
    "    temp1.show()\n",
    "    \n",
    "\n",
    "    scaledData = temp1.withColumn(\"timestamp1\", to_timestamp(\"timestamp\", 'yyyy-MM-dd HH:mm:ss')).cache()\n",
    "    \n",
    "    movAvg = movingAverage(scaledData)\n",
    "    movAvg.createOrReplaceTempView(\"movAvg\")\n",
    "    \n",
    "    #Select only rounded values\n",
    "    scaledNorm =  spark.sql('SELECT timestamp1,Z,Zscale,round(movingAverage,0) as movingAverage_round from movAvg').cache()\n",
    "    \n",
    "    #Select only rounded values\n",
    "   \n",
    "    print(\"Scaled Data\\n\")\n",
    "    scaledNorm.show()\n",
    "    scaledNorm.createOrReplaceTempView(\"scaledNorm\") \n",
    "    \n",
    "    #specify the metric column to be modelled\n",
    "    data= spark.sql(\"select cast(movingAverage_round as decimal)  as  movingAverage_round from scaledNorm\").cache()\n",
    "    data.createOrReplaceTempView(\"data\") \n",
    "\n",
    "    features = (VectorAssembler(inputCols=data.columns, outputCol=\"features\").transform(data).select(\"features\"))\n",
    "\n",
    "    features.show()\n",
    "    print(features.count())\n",
    "    features.createOrReplaceTempView(\"features\")\n",
    "    \n",
    "    \n",
    "    #Iforest model\n",
    "    model_val = iforest_model(features).cache()\n",
    "\n",
    "    model_val.createOrReplaceTempView(\"model_val\") \n",
    "    model_val.show()\n",
    "    print(\"#(1.0 means anomalous/ outlier, 0.0 normal/ inlier)\")\n",
    "    print(\"Now in main\\n\")\n",
    "    count_anomalies= spark.sql('select prediction,count(prediction) from model_val group by prediction')\n",
    "    count_anomalies.show()\n",
    "    \n",
    "    \n",
    "    to_array = functions.udf(lambda v: v.toArray().tolist(), types.ArrayType(types.FloatType()))\n",
    "    test_df = model_val.withColumn('MvngAvg_feature', to_array('features'))\n",
    "    test_df = test_df.withColumn(\"movingAverage_round\", test_df[\"MvngAvg_feature\"].getItem(0))\n",
    "    test_df.createOrReplaceTempView(\"test_df\")  \n",
    "    \n",
    "    test_df = spark.sql('select movingAverage_round, prediction, anomalyScore from test_df').cache()\n",
    "    test_df.createOrReplaceTempView(\"test_df\") \n",
    "    test_df.show()\n",
    "    \n",
    "#     test_df.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"iForest_Out\")\n",
    "   \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    inputs = 'Lift1_sample.csv'\n",
    "    main(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
